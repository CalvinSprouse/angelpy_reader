{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorer\n",
    "\n",
    "Design the angelpy web scraper.\n",
    "\n",
    "#### Alternative Concept\n",
    "A numpy array of chapters containing two integer columns, page and post,\n",
    "could replace the list of dicts with lists.\n",
    "All it would require is functions to parse the numbers from the strings then recreate them.\n",
    "Would be more complicated and mildly less flexible as it would require the base url be saved\n",
    "then assuming all storied from the same threadmark page are on the same thread,\n",
    "which is a reasonable assumption.\n",
    "There would not be many benefits to this approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urljoin, urldefrag, ParseResult\n",
    "\n",
    "import html2text\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from rich import print\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to parse tla style urls\n",
    "def parse_tla_url(tla_url: str) -> dict:\n",
    "\n",
    "    # parse the tla url\n",
    "    parsed_tla = urlparse(tla_url)\n",
    "\n",
    "\n",
    "    # extract the page number\n",
    "    # the default page number will be 1\n",
    "    page_number = 1\n",
    "\n",
    "    # create a regular expression for extracting the page number\n",
    "    # the page number comes at the end of the url of the form\n",
    "    # page-# where # is any digit page number\n",
    "    page_number_regex = re.compile(r'page-(\\d+)$')\n",
    "\n",
    "    # extract the page number from parsed_tla.path\n",
    "    # if none exists default to one\n",
    "    page_number_match = page_number_regex.search(parsed_tla.path)\n",
    "    if page_number_match:\n",
    "        page_number = int(page_number_match.group(1))\n",
    "\n",
    "\n",
    "    # extract the post id as an integer from the fragment using regular expression\n",
    "    # the post id comes at the end of the url of the form\n",
    "    # post-# where # is any digit page number\n",
    "    post_id_regex = re.compile(r'post-(\\d+)$')\n",
    "\n",
    "    # extract the post id from parsed_tla.fragment\n",
    "    post_id_match = post_id_regex.search(parsed_tla.fragment)\n",
    "    post_id = int(post_id_match.group(1))\n",
    "\n",
    "\n",
    "    # return the page and post as a dict\n",
    "    return {'page': page_number, 'post': post_id}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to compose tla style urls\n",
    "def compose_tla_url(base_url: str, page: int, post: int) -> ParseResult:\n",
    "\n",
    "    # take the page and posts and turn them into strings\n",
    "    page_str = f\"page-{page}\"\n",
    "    post_str = f\"#post-{post}\"\n",
    "    joint_str = f\"{page_str}/{post_str}\"\n",
    "\n",
    "    # compose a joined url from urljoin\n",
    "    joint_url = urljoin(base_url, joint_str)\n",
    "\n",
    "    # return a parsed joint url\n",
    "    joint_parse_url = urlparse(joint_url)\n",
    "    return joint_parse_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get soup from a link\n",
    "def make_soup(link: str, headers: dict = None, pause_interval: float = None) -> BeautifulSoup:\n",
    "    \"\"\"Makes a soup from a link.\n",
    "\n",
    "    Arguments:\n",
    "        link -- the link to make the soup from (str).\n",
    "\n",
    "    Keyword Arguments:\n",
    "        headers -- the headers to use for the request (default: User-Agent: Mozilla/5.0).\n",
    "\n",
    "    Returns:\n",
    "        soup -- the soup from the link (BeautifulSoup).\n",
    "    \"\"\"\n",
    "\n",
    "    # create default headers if none were passed\n",
    "    if headers is None:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    # if no pause interval was passed pause for a random interval to avoid\n",
    "    # overloading websites\n",
    "    if pause_interval is None: pause_interval = random.uniform(0.5, 1.5)\n",
    "\n",
    "    time.sleep(pause_interval)\n",
    "\n",
    "    # create a requests session to handle webpage data\n",
    "    session = requests.Session()\n",
    "\n",
    "    # get a response from the main page\n",
    "    response = session.get(link, headers=headers)\n",
    "\n",
    "    # check the response code and raise an error if not good\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Bad response code: {response.status_code}\")\n",
    "\n",
    "    # parse the soup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # return the soup\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to parse a threadmark page\n",
    "def parse_threadmark_page(threadmark_link: str) -> np.array:\n",
    "    \"\"\"Parse a threadmark page (table of contents) and return chapter links.\n",
    "\n",
    "    Arguments:\n",
    "        threadmark_link -- A link to the threadmark page, ends in /threadmarks\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with keys\n",
    "        \"href\": link to chapter,\n",
    "        \"post\": the post number,\n",
    "        \"title\": the text accompanying the link\n",
    "    \"\"\"\n",
    "\n",
    "    # define a link to a threadmark page\n",
    "    # threadmark_link = str(tla_links[0].get(\"href\")) + \"threadmarks\"\n",
    "    # print(f\"Threadmark: '{threadmark_link}'\")\n",
    "\n",
    "    # parse out the link\n",
    "    # parsed_link = urlparse(threadmark_link)\n",
    "\n",
    "    # get the soup of a threadmark page\n",
    "    thread_soup = make_soup(threadmark_link)\n",
    "\n",
    "    # get the threadmark containers\n",
    "    # each container has a link to a threadmark\n",
    "    threadmark_containers = thread_soup.find_all(attrs={\"class\": \"structItem--threadmark\"})\n",
    "\n",
    "    # extract all the links from threadmark containers\n",
    "    # the first link is from the actual link and has the text, the second has the date\n",
    "    # so take only the first\n",
    "    thread_links = [l.find_all(\"a\")[0] for l in threadmark_containers]\n",
    "\n",
    "    # clean the thread links\n",
    "    # first parse into a list of lists (parsed link, link label)\n",
    "    # clean_thread_links = [(urlparse(l.get(\"href\")), l.get_text().strip()) for l in thread_links]\n",
    "\n",
    "    # compose full links from the urlparse with the base scheme and netloc\n",
    "    # save as a list of tuples of the form (link, post code, link label)\n",
    "    # use url join from urlparse?\n",
    "    parsed_thread_links = np.array(\n",
    "        [(\n",
    "            (parsed_url := parse_tla_url(l.get(\"href\")))[\"page\"],\n",
    "            parsed_url[\"post\"],\n",
    "            l.get_text()\n",
    "        ) for l in thread_links],\n",
    "        dtype=[\n",
    "            (\"page\", np.int32),\n",
    "            (\"post\", np.int64),\n",
    "            (\"title\", np.object_)\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    # return the thread links\n",
    "    return parsed_thread_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to parse a chapter from above methods\n",
    "def parse_entry(base_url: str, page: int, post: int) -> str:\n",
    "\n",
    "    # compose the entry url\n",
    "    entry_url = compose_tla_url(base_url, page, post)\n",
    "\n",
    "    # get the soup\n",
    "    entry_soup = make_soup(entry_url.geturl())\n",
    "\n",
    "    # get the first div element with data-lb-id=post-id\n",
    "    post_element = entry_soup.find(\"div\", {\"data-lb-id\": entry_url.fragment})\n",
    "\n",
    "    # go in one layer and find the class=bbWrapper tag\n",
    "    post_wrapper = post_element.find(\"div\", class_=\"bbWrapper\")\n",
    "\n",
    "    # unfortunately due to mixed formatting I think it just has to be converted to raw text\n",
    "    entry_raw_text = str(post_wrapper)\n",
    "\n",
    "    # pass throught the html to text parser and fix some issues with '_'\n",
    "    # parse using html2text\n",
    "    html_parser = html2text.HTML2Text()\n",
    "    entry_text = html_parser.handle(entry_raw_text)\n",
    "\n",
    "    # if a number of asterix are seperated from another group of asterix by nothing but blank space\n",
    "    # characters remove the blank space characters and the asterix\n",
    "    # example **text** **more text** would become **textmore text**\n",
    "    # but there might be any number of asterix that need to be removed\n",
    "    # this is done to make the regex for finding the entry titles easier\n",
    "    entry_text = re.sub(r\"\\*\\*\\s+\\*\\*\", \" \", entry_text)\n",
    "    return entry_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Info on Each Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the links from the main webpage\n",
    "\n",
    "# the links will have associated text The Last Angel and come from this page\n",
    "main_page = r\"https://proximalflame.com/index-2/\"\n",
    "\n",
    "# get a response from the main page and extract the html\n",
    "soup = make_soup(main_page)\n",
    "\n",
    "# extract all links from the main page\n",
    "links = list(soup.find_all(\"a\"))\n",
    "\n",
    "# get only links with text The Last Angel\n",
    "# these are the links to the main stories\n",
    "# appending threadmarks to the end of these links gives a table of contents of sorts\n",
    "# this works for now but is a little messy\n",
    "tla_links = [\n",
    "    {\n",
    "        \"href\": (href := urldefrag(l.get(\"href\"))[0]),\n",
    "        \"title\": l.get_text().strip(),\n",
    "        \"entrys\": parse_threadmark_page(urljoin(href, \"threadmarks\"))\n",
    "    }\n",
    "    for l in links if \"the last angel\" in l.get_text().strip().lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse a 'Book'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Thread 'The Last Angel':   0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Thread 'The Last Angel': 100%|##########| 55/55 [01:30<00:00,  1.64s/it]\n",
      "Parsing Thread 'The Last Angel: Ascension': 100%|##########| 92/92 [02:29<00:00,  1.63s/it]\n",
      "Parsing Thread 'The Last Angel: The Hungry Stars':  75%|#######5  | 45/60 [01:16<00:25,  1.70s/it]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Bad response code: 429",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m entry_file \u001b[39m=\u001b[39m thread_dir \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mentry[\u001b[39m2\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[39m# run parse entry\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m entry_text \u001b[39m=\u001b[39m parse_entry(thread[\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m], entry[\u001b[39m0\u001b[39m], entry[\u001b[39m1\u001b[39m])\n\u001b[1;32m     27\u001b[0m \u001b[39m# save the entry text to a file\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(entry_file, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f: f\u001b[39m.\u001b[39mwrite(entry_text)\n",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m, in \u001b[0;36mparse_entry\u001b[0;34m(base_url, page, post)\u001b[0m\n\u001b[1;32m      5\u001b[0m entry_url \u001b[39m=\u001b[39m compose_tla_url(base_url, page, post)\n\u001b[1;32m      7\u001b[0m \u001b[39m# get the soup\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m entry_soup \u001b[39m=\u001b[39m make_soup(entry_url\u001b[39m.\u001b[39mgeturl())\n\u001b[1;32m     10\u001b[0m \u001b[39m# get the first div element with data-lb-id=post-id\u001b[39;00m\n\u001b[1;32m     11\u001b[0m post_element \u001b[39m=\u001b[39m entry_soup\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mdata-lb-id\u001b[39m\u001b[39m\"\u001b[39m: entry_url\u001b[39m.\u001b[39mfragment})\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mmake_soup\u001b[0;34m(link, headers, pause_interval)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m# check the response code and raise an error if not good\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBad response code: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[39m# parse the soup\u001b[39;00m\n\u001b[1;32m     36\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mcontent, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Bad response code: 429"
     ]
    }
   ],
   "source": [
    "# iterate over each selection in tla_links\n",
    "\n",
    "# define a save location for all output\n",
    "output_dir = Path(\".output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# each selection is a thread\n",
    "for thread in tla_links:\n",
    "    # define a location for the thread\n",
    "    thread_dir = output_dir / thread['title'].lower().replace(\" \", \"_\").replace(\":\", \"-\")\n",
    "    thread_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # iterate over each entry in the thread\n",
    "    for i, entry in enumerate(tqdm(\n",
    "            thread[\"entrys\"],\n",
    "            desc=f\"Parsing Thread '{thread['title']}'\",\n",
    "            ascii=True,\n",
    "            leave=True,\n",
    "            position=0)):\n",
    "\n",
    "        # define the save location of the entry\n",
    "        entry_file = thread_dir / f\"{entry[2]}.txt\"\n",
    "\n",
    "        # check if the entry exists, if it does don't redownload\n",
    "        if entry_file.exists(): continue\n",
    "\n",
    "        # run parse entry because entry does not exist\n",
    "        entry_text = parse_entry(thread['href'], entry[0], entry[1])\n",
    "\n",
    "        # save the entry text to a file\n",
    "        with open(entry_file, \"w\", encoding=\"utf-8\") as f: f.write(entry_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
