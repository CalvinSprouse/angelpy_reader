{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorer\n",
    "\n",
    "Design the angelpy web scraper.\n",
    "\n",
    "#### Alternative Concept\n",
    "A numpy array of chapters containing two integer columns, page and post,\n",
    "could replace the list of dicts with lists.\n",
    "All it would require is functions to parse the numbers from the strings then recreate them.\n",
    "Would be more complicated and mildly less flexible as it would require the base url be saved\n",
    "then assuming all storied from the same threadmark page are on the same thread,\n",
    "which is a reasonable assumption.\n",
    "There would not be many benefits to this approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urljoin, urldefrag, ParseResult\n",
    "\n",
    "import html2text\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from rich import print\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get soup from a link\n",
    "def make_soup(link: str, headers: dict = None, pause_interval: float = None) -> BeautifulSoup:\n",
    "    \"\"\"Makes a soup from a link.\n",
    "\n",
    "    Arguments:\n",
    "        link -- the link to make the soup from (str).\n",
    "\n",
    "    Keyword Arguments:\n",
    "        headers -- the headers to use for the request (default: User-Agent: Mozilla/5.0).\n",
    "\n",
    "    Returns:\n",
    "        soup -- the soup from the link (BeautifulSoup).\n",
    "    \"\"\"\n",
    "\n",
    "    # create default headers if none were passed\n",
    "    if headers is None:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    # if no pause interval was passed pause for a random interval to avoid\n",
    "    # overloading websites\n",
    "    if pause_interval is None: pause_interval = random.uniform(0.5, 1.5)\n",
    "\n",
    "    time.sleep(pause_interval)\n",
    "\n",
    "    # create a requests session to handle webpage data\n",
    "    session = requests.Session()\n",
    "\n",
    "    # get a response from the main page\n",
    "    response = session.get(link, headers=headers)\n",
    "\n",
    "    # check the response code and raise an error if not good\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Bad response code: {response.status_code}\")\n",
    "\n",
    "    # parse the soup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # return the soup\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to parse a threadmark page\n",
    "def parse_threadmark_page(threadmark_link: str) -> dict:\n",
    "    \"\"\"Parse a threadmark page (table of contents) and return chapter links.\n",
    "\n",
    "    Arguments:\n",
    "        threadmark_link -- A link to the threadmark page, ends in /threadmarks\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with keys\n",
    "        \"url\": link to chapter,\n",
    "        \"label\": the text accompanying the link\n",
    "    \"\"\"\n",
    "\n",
    "    # define a link to a threadmark page\n",
    "    # threadmark_link = str(tla_links[0].get(\"href\")) + \"threadmarks\"\n",
    "    # print(f\"Threadmark: '{threadmark_link}'\")\n",
    "\n",
    "    # parse out the link\n",
    "    # parsed_link = urlparse(threadmark_link)\n",
    "\n",
    "    # get the soup of a threadmark page\n",
    "    thread_soup = make_soup(threadmark_link)\n",
    "\n",
    "    # get the threadmark containers\n",
    "    # each container has a link to a threadmark\n",
    "    threadmark_containers = thread_soup.find_all(attrs={\"class\": \"structItem--threadmark\"})\n",
    "\n",
    "    # extract all the links from threadmark containers\n",
    "    # the first link is from the actual link and has the text, the second has the date\n",
    "    # so take only the first\n",
    "    thread_links = [l.find_all(\"a\")[0] for l in threadmark_containers]\n",
    "\n",
    "    # clean the thread links\n",
    "    # first parse into a list of lists (parsed link, link label)\n",
    "    # clean_thread_links = [(urlparse(l.get(\"href\")), l.get_text().strip()) for l in thread_links]\n",
    "\n",
    "    # compose full links from the urlparse with the base scheme and netloc\n",
    "    # save as a list of dicts with url and title\n",
    "    parsed_thread_links = [{\n",
    "        \"url\": urlparse(urljoin(threadmark_link, l.get(\"href\"))),\n",
    "        \"label\": l.get_text().strip()\n",
    "        } for l in thread_links]\n",
    "\n",
    "    # return the thread links\n",
    "    return parsed_thread_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Info on Each Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing TLA Links: 100%|██████████| 3/3 [00:04<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "# scrape the links from the main webpage\n",
    "\n",
    "# the links will have associated text The Last Angel and come from this page\n",
    "main_page = r\"https://proximalflame.com/index-2/\"\n",
    "\n",
    "# get a response from the main page and extract the html\n",
    "soup = make_soup(main_page)\n",
    "\n",
    "# extract all links from the main page\n",
    "links = list(soup.find_all(\"a\"))\n",
    "\n",
    "# get only links that correspond to TLA stories\n",
    "tla_threadlinks = [l for l in links if \"the last angel\" in l.get_text().strip().lower()]\n",
    "\n",
    "# get only links with text The Last Angel\n",
    "# these are the links to the main stories\n",
    "# appending threadmarks to the end of these links gives a table of contents of sorts\n",
    "# this works for now but is a little messy\n",
    "tla_links = [\n",
    "    {\n",
    "        \"url\": (url := urlparse(l.get(\"href\"), allow_fragments=False)),\n",
    "        \"label\": l.get_text().strip(),\n",
    "        \"entrys\": parse_threadmark_page(urljoin(url.geturl(), \"threadmarks\"))\n",
    "    }\n",
    "    for l in tqdm(\n",
    "        tla_threadlinks,\n",
    "        desc=\"Parsing TLA Links\"\n",
    "    )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse a 'Book'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work on the parse_entry Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the entry (to be a function later)\n",
    "def parse_entry(entry_url: str, entry_post_str: str) -> list:\n",
    "    # first get the entry soup\n",
    "    entry_soup = make_soup(entry_url)\n",
    "\n",
    "    # get the post element first by post then by bbWrapper class\n",
    "    post_container = entry_soup.find(\"div\", {\"data-lb-id\": entry_post_str})\n",
    "    post = post_container.find(\"div\", class_=\"bbWrapper\")\n",
    "\n",
    "    # convert the post to a str\n",
    "    html_parser = html2text.HTML2Text()\n",
    "    post_str = html_parser.handle(str(post))\n",
    "\n",
    "    # list the chapter selection choices\n",
    "    chapter_choices = set([p.get_text().strip() for p in post.find_all(\"b\")])\n",
    "    # print(f\"choices: {chapter_choices}.\")\n",
    "\n",
    "    # if there are no choices return none\n",
    "    if len(chapter_choices) == 0: return None\n",
    "\n",
    "    # define a regular expression to parse the set\n",
    "    chapter_re = re.compile(\n",
    "        r\"(chapter|epilogue|prologue|interrupt|pt|the\\slast\\sangel)\",\n",
    "        re.IGNORECASE)\n",
    "\n",
    "    # keep only set entries which are matched to the chapter_re\n",
    "    chapter_choices_rem = set(filter(chapter_re.search, chapter_choices))\n",
    "    # print(f\"remaining: {chapter_choices}.\")\n",
    "\n",
    "    # if no choices remain check if there was only one option\n",
    "    # if so that becomes the set of remaining choices\n",
    "    # if there were no options to begin with return None\n",
    "    if len(chapter_choices_rem) == 0 and len(chapter_choices) == 1:\n",
    "        chapter_choices_rem = set(chapter_choices)\n",
    "\n",
    "    # determine the indices of each chapter choice\n",
    "    # and determine which line from post_lines\n",
    "    chapter_str_indices = [post_str.find(chapter) for chapter in chapter_choices_rem]\n",
    "    # print(f\"indices: {chapter_str_indices}.\")\n",
    "\n",
    "    # convert the str index to a line number\n",
    "    chapter_line_indices = [post_str[:index].count(\"\\n\") for index in chapter_str_indices]\n",
    "\n",
    "    # use the indices to seperate the string into parts\n",
    "    post_lines = post_str.split(\"\\n\")\n",
    "    chapter_lines = [post_lines[index:] for index in chapter_line_indices]\n",
    "\n",
    "    # ensure the remaining choices and chapter lines lists are the same length\n",
    "    assert len(chapter_choices_rem) == len(chapter_lines)\n",
    "\n",
    "    # return a list of tuples where the first entry is the choice and the second entry is the list\n",
    "    chapters = list(zip(chapter_choices_rem, chapter_lines))\n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 'The Last Angel': 100%|██████████| 55/55 [01:19<00:00,  1.44s/it]\n",
      "Reading 'The Last Angel: Ascension':  32%|███▏      | 29/92 [00:42<01:43,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (None): 'The Last Angel: Ascension'/'Re: Spoilers'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 'The Last Angel: Ascension':  39%|███▉      | 36/92 [00:53<01:22,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (len=0): 'The Last Angel: Ascension'/'Askanj Government'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 'The Last Angel: Ascension':  47%|████▋     | 43/92 [01:04<01:20,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (None): 'The Last Angel: Ascension'/'Map of Galhemna System'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 'The Last Angel: Ascension':  57%|█████▋    | 52/92 [01:17<01:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (len=0): 'The Last Angel: Ascension'/'Stillness'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 'The Last Angel: Ascension':  92%|█████████▏| 85/92 [02:03<00:09,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (None): 'The Last Angel: Ascension'/'Hungry Stars Prologue'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 'The Last Angel: Ascension':  99%|█████████▉| 91/92 [02:12<00:01,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (None): 'The Last Angel: Ascension'/'The Hungry Stars teaser'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 'The Last Angel: Ascension': 100%|██████████| 92/92 [02:13<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (None): 'The Last Angel: Ascension'/'The Hungry Stars'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 'The Last Angel: The Hungry Stars':   2%|▏         | 1/61 [00:01<01:48,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (None): 'The Last Angel: The Hungry Stars'/'Table of Contents'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 'The Last Angel: The Hungry Stars':  18%|█▊        | 11/61 [00:15<01:09,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (None): 'The Last Angel: The Hungry Stars'/'Species Naming Traditions'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 'The Last Angel: The Hungry Stars':  74%|███████▍  | 45/61 [01:03<00:24,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (len=0): 'The Last Angel: The Hungry Stars'/'Pets'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 'The Last Angel: The Hungry Stars': 100%|██████████| 61/61 [01:29<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "# configure book saving\n",
    "# define an output directory\n",
    "output_dir = Path(\".output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# loop over each book\n",
    "for bi, book in enumerate(tla_links):\n",
    "    # extract book information\n",
    "    book_title = book[\"label\"]\n",
    "    # print(f\"Reading '{book_title}'.\")\n",
    "\n",
    "    # define the books output directory\n",
    "    book_output_dir = output_dir / f\"book_{bi}\"\n",
    "    book_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # loop over each entry\n",
    "    for i, entry in enumerate((\n",
    "            pbar := tqdm(\n",
    "                book[\"entrys\"],\n",
    "                desc=f\"Reading '{book_title}'\"\n",
    "            ))):\n",
    "\n",
    "        # extract the entry\n",
    "        entry_title = entry['label']\n",
    "        entry_url = entry['url'].geturl()\n",
    "        entry_post_str = entry['url'].fragment\n",
    "\n",
    "        # extract the entry\n",
    "        entry_extract = parse_entry(entry_url, entry_post_str)\n",
    "\n",
    "        # check that entry_extract is not None and has len > 0\n",
    "        # print what book/entry but continue otherwise\n",
    "        if entry_extract is None:\n",
    "            print(f\"Error (None): '{book_title}'/'{entry_title}'\")\n",
    "            continue\n",
    "        if len(entry_extract) == 0:\n",
    "            print(f\"Error (len=0): '{book_title}'/'{entry_title}'\")\n",
    "            continue\n",
    "\n",
    "        # loop over each entry and write to a file\n",
    "        # chapter is a tuple of title and text\n",
    "        # text is a list of lines\n",
    "        for ci, chapter in enumerate(entry_extract):\n",
    "            # save the chapter text to a text file\n",
    "            chapter_output_file = book_output_dir / f\"chapt_{i}_{ci}.txt\"\n",
    "\n",
    "            # write the chapter text to a file\n",
    "            with open(chapter_output_file, \"w\") as f:\n",
    "                f.write(\"\\n\".join(chapter[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
